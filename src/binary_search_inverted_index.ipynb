{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c58f8680",
   "metadata": {},
   "source": [
    "# Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddc0047",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import re\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3205a41",
   "metadata": {},
   "source": [
    "# Configuración de parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdae170",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d085fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "pattern = r'''(?x)              \n",
    "    (?:[A-Z]\\.)+[A-Z]?          # abreviaturas, e.g. U.S.A.\n",
    "  | [A-Za-z]+(?:-[A-Za-z]+)*    # palabras con guiones internos, e.g. non-euclidean\n",
    "  | \\$?\\d+(?:\\.\\d+)?%?          # monedas, números, porcentajes\n",
    "  | \\.\\.\\.                      # puntos suspensivos\n",
    "  | [][.,;\"'?():_`-]            # puntuación como tokens separados\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fd4b9c",
   "metadata": {},
   "source": [
    "# Proceso de preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7bbf8bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text: str) -> List[str]:\n",
    "    # Tokenización del texto a nivel de palabra usando expresión regular para inglés\n",
    "    tokens = regexp_tokenize(text, pattern)\n",
    "    # Normalización de los tokens\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    # Eliminación de palabras de parada\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Stemming de los tokens\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    # Retornar tokens\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ab222820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['se', 'deben', 'invertir', '$12.000', 'millon', 'en', 'd.i.a', '...']\n"
     ]
    }
   ],
   "source": [
    "print(preprocess(\"Se deben invertir $12.000 millones en D.I.A ...\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5cca6d",
   "metadata": {},
   "source": [
    "# Ingesta de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c5e1e57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta en la que se encuentran los documentos a procesar\n",
    "path_docs = \"../data/docs-raw-texts\"\n",
    "\n",
    "def load_documents(path_docs: str):\n",
    "\t# Diccionario para almacenar los documentos por id\n",
    "\tdocs = {}\n",
    "\t# Se revisan únicamente los archivos relevantes\n",
    "\tfor file_name in os.listdir(path_docs):\n",
    "\t\tif not file_name.endswith(\".naf\"):\n",
    "\t\t\tcontinue\n",
    "\t\t# Construccion del arbol de atributos del documento\n",
    "\t\ttree = ET.parse(os.path.join(path_docs, file_name))\n",
    "\t\troot = tree.getroot()\n",
    "\t\t# Id recuperado desde el atributo pupblicId\n",
    "\t\tdoc_id = root.find(\"nafHeader/public\").attrib[\"publicId\"]\n",
    "\t\t# Titulo desde el atributo title\n",
    "\t\ttitle = root.find(\"nafHeader/fileDesc\").attrib.get(\"title\", \"\")\n",
    "\t\t# Contenido desde <raw>\n",
    "\t\traw_element = root.find(\"raw\")\n",
    "\t\tcontent = raw_element.text if raw_element is not None else \"\"\n",
    "\t\t# concatenar titulo + contenido para preprocesar\n",
    "\t\ttokens = preprocess(title +  \" \" + content)\n",
    "\t\tdocs[doc_id] = tokens\n",
    "\t# Retornar los documentos cargados\n",
    "\treturn docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d3d6ef18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eugenio', 'beltrami', 'non-euclidian', 'geometri', 'eugenio', 'beltrami', 'non-euclidian', 'geometri', '.', 'eugenio', 'beltrami', '(', '1835', '-', '1900', ')', '.', 'novemb', '16', ',', '1835', ',', 'italian', 'mathematician', 'eugenio', 'beltrami', 'born', '.', 'notabl', 'work', 'concern', 'differenti', 'geometri', 'mathemat', 'physic', '.', 'work', 'note', 'especi', 'clariti', 'exposit', '.', 'first', 'prove', 'consist', 'non-euclidean', 'geometri', 'model', 'surfac', 'constant', 'curvatur', ',', 'pseudospher', '.', 'eugenio', 'beltrami', 'born', 'cremona', 'lombardi', ',', 'part', 'austrian', 'empir', ',', 'part', 'itali', '.', 'son', 'artist', 'paint', 'miniatur', ',', 'young', 'eugenio', 'certain', 'inherit', 'artist', 'talent', 'famili', ',', 'case', 'addit', 'mathemat', 'talent', 'would', 'acquir', ',', 'musicrath', 'paint', 'becam', 'import', 'life', '.', 'began', 'studi', 'mathemat', 'univers', 'pavia', '1853', ',', 'expel', 'ghislieri', 'colleg', '1856', 'due', 'polit', 'opinion', '.', 'time', 'taught', 'influenc', 'francesco', 'brioschi', ',', 'appoint', 'professor', 'appli', 'mathemat', 'univers', 'pavia', 'year', 'beltrami', 'began', 'studi', '.', 'beltrami', 'discontinu', 'studi', 'financi', 'hardship', 'spent', 'next', 'sever', 'year', 'secretari', 'work', 'lombardi', 'venic', 'railroad', 'compani', 'first', 'verona', 'later', 'milan', '.', 'beltrami', 'milan', 'kingdom', 'itali', 'establish', '1861', ',', 'import', 'polit', 'event', 'much', 'invigor', 'academ', 'scene', 'itali', '.', 'beltrami', 'began', 'work', 'hard', 'mathemat', 'studi', '1862', 'publish', 'first', 'paper', '.', 'result', ',', 'appoint', 'univers', 'bologna', 'professor', '1862', '.', '1870', ',', 'new', 'univers', 'rome', 'set', 'new', 'italian', 'capit', 'beltrami', 'appoint', 'chair', 'ration', 'mechan', '1873', '.', 'three', 'year', 'rome', ',', 'beltrami', 'move', 'pavia', 'take', 'chair', 'mathemat', 'physic', '.', 'howev', ',', 'beltrami', 'return', 'rome', '1891', 'spent', 'last', 'year', 'teach', '.', '[', '1', ']', 'becam', 'presid', 'accademia', 'dei', 'lincei', '1898', ',', 'follow', 'year', ',', 'senat', 'kingdom', '.', 'lover', 'music', ',', 'beltrami', 'interest', 'relationship', 'mathemat', 'music', '.', '[', '2', ']', 'm.c.e', 'scher', ',', 'circl', 'limit', 'iv', ',', 'illustr', 'hyperbol', 'geometri', '1868', 'beltrami', 'publish', 'two', 'memoir', 'deal', 'consist', 'interpret', 'non-euclidean', 'geometri', 'bolyai', 'lobachevski', '.', 'beltrami', 'propos', 'geometri', 'could', 'realiz', 'surfac', 'constant', 'negat', 'curvatur', ',', 'pseudospher', '.', 'beltrami', 'concept', ',', 'line', 'geometri', 'repres', 'geodes', 'pseudospher', 'theorem', 'non-euclidean', 'geometri', 'prove', 'within', 'ordinari', 'three-dimension', 'euclidean', 'space', ',', 'deriv', 'axiomat', 'fashion', ',', 'lobachevski', 'bolyai', 'done', 'previous', '.', 'alreadi', '1840', ',', 'mind', 'alreadi', 'consid', 'geodes', 'triangl', 'pseudospher', 'remark', 'correspond', 'trigonometr', 'formula', 'obtain', 'correspond', 'formula', 'spheric', 'trigonometri', 'replac', 'usual', 'trigonometr', 'function', 'hyperbol', 'function', 'second', 'memoir', 'fundament', 'theori', 'space', 'constant', 'curvatur', ',', 'beltrami', 'continu', 'logic', 'gave', 'abstract', 'proof', 'equiconsist', 'hyperbol', 'euclidean', 'geometri', 'dimens', '.', 'accomplish', 'introduc', 'sever', 'model', 'non-euclidean', 'geometri', 'known', 'beltrami', 'klein', 'model', ',', 'poincar', 'disk', 'model', ',', 'poincar', 'half-plan', 'model', ',', 'togeth', 'transform', 'relat', '.', 'although', 'today', 'beltrami', 'essay', 'recogn', 'import', 'develop', 'non-euclidean', 'geometri', ',', 'recept', 'time', 'less', 'enthusiast', '.', 'beltrami', 'also', 'work', 'optic', ',', 'thermodynam', ',', 'elast', ',', 'electr', 'magnet', '.', 'contribut', 'topic', 'appear', 'four-volum', 'work', ',', 'oper', 'matematich', '(', '1902', '-', '20', ')', ',', 'publish', 'posthum', '.', 'yovisto', ',', 'learn', 'non-euclidian', 'geometri', 'histori', 'mathemat', 'lectur', 'professor', 'n.', 'j.', 'wildberg', 'mathhist', '12', 'non-euclidian', 'geometri', '.']\n"
     ]
    }
   ],
   "source": [
    "documents = load_documents(path_docs)\n",
    "print(documents[\"d006\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d0aabf",
   "metadata": {},
   "source": [
    "# Construcción de índice invertido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f6c1ea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inverted_index(docs):\n",
    "    # Construcción de la estructura de indice invertido\n",
    "    inverted = defaultdict(set)\n",
    "    # Iteración sobre los doumentos cargados\n",
    "    for doc_id, tokens in docs.items():\n",
    "        # Iteración sobre los tokens de los documentos\n",
    "        for token in tokens:\n",
    "            # Población del índice\n",
    "            inverted[token].add(doc_id)\n",
    "    # Convertir sets a listas ordenadas\n",
    "    for token in inverted:\n",
    "        inverted[token] = {\"df\" : len(inverted[token]), \"postings\" : sorted(inverted[token])}\n",
    "    # Se retorna el índice construido\n",
    "    return inverted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "908bedae",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = build_inverted_index(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea18267",
   "metadata": {},
   "source": [
    "# Calculo de consultad mediante algoritmo de mezcla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8d832c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and(postings_1, postings_2):\n",
    "\t# Inicialización de los punteros\n",
    "\ti, j = 0, 0\n",
    "\tresult = []\n",
    "\t# Busqueda mediante la estrategia de dos punteros\n",
    "\twhile i < len(postings_1) and j < len(postings_2):\n",
    "\t\t# Si los valores son iguales, se cumple el and\n",
    "\t\tif postings_1[i] == postings_2[j]:\n",
    "\t\t\tresult.append(postings_1[i])\n",
    "\t\t\ti += 1\n",
    "\t\t\tj += 1\n",
    "\t\t# Se avanza el puntero que apunta al documento con menor doc_id\n",
    "\t\telif postings_1[i] < postings_2[j]:\n",
    "\t\t\ti += 1\n",
    "\t\telse:\n",
    "\t\t\tj += 1\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "80c12e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_not(postings, all_docs):\n",
    "\t# Inicialización de los punteros\n",
    "\tresult = []\n",
    "\ti, j = 0, 0\n",
    "\t# Busqueda mediante la estrategia de dos punteros\n",
    "\twhile i < len(all_docs) and j < len(postings):\n",
    "\t\t# Si el termino esta tanto en el posting como en la lista de todos los documentos,\n",
    "\t\t# no se cumple el not\n",
    "\t\tif all_docs[i] == postings[j]:\n",
    "\t\t\ti += 1\n",
    "\t\t\tj += 1\n",
    "\t\t# Se cumple el not y se agrega\n",
    "\t\telif all_docs[i] < postings[j]:\n",
    "\t\t\tresult.append(all_docs[i])\n",
    "\t\t\ti += 1\n",
    "\t\telse:\n",
    "\t\t\tj += 1\n",
    "\t# Se agregan todos los no evaluados porque cumplen el not\n",
    "\tresult.extend(all_docs[i:])\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7a4fbd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_query(query_tokens, index, all_docs):\n",
    "    stack = []\n",
    "    for token in query_tokens:\n",
    "        if token == \"AND\":\n",
    "            postings_1 = stack.pop()\n",
    "            postings_2 = stack.pop()\n",
    "            stack.append(merge_and(postings_1, postings_2))\n",
    "        elif token == \"NOT\":\n",
    "            postings = stack.pop()\n",
    "            stack.append(merge_not(postings, all_docs))\n",
    "        else:\n",
    "            postings = index.get(token, {\"postings\": []})[\"postings\"]\n",
    "            stack.append(postings)\n",
    "    return stack.pop() if stack else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8a96d296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_query(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    return root.find(\"raw\").text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "99e651bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fabrication of music instruments\n"
     ]
    }
   ],
   "source": [
    "print(read_query(\"../data/queries-raw-texts/wes2015.q01.naf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd96e698",
   "metadata": {},
   "outputs": [],
   "source": [
    "## def process_queries(path_queries, index, all_docs, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
